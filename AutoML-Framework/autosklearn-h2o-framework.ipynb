{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install auto-sklearn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ucimlrepo","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#making imports\nimport autosklearn.classification\nimport sklearn.model_selection\nfrom sklearn import preprocessing\nfrom sklearn.datasets import fetch_openml\nfrom ucimlrepo import fetch_ucirepo\nimport h2o\nimport pandas as pd\nimport threading\nimport time\nfrom colorama import Fore","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#creating frame for storing result\nresults = pd.DataFrame(columns=[\"ID\", \"DataSet\", \"AutoML lib.\", \"Time lim.\", \"Algorithm\", \"Accuracy\", \"Precision\", \"Recall\", \"F-measure\", \"AUC\", \"Time\"])\ni=0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Info:\n    def __init__(self, data, target, frame, feature_names): #fields for reading from file\n        self.data = data\n        self.target = target\n        self.frame = frame\n        self.feature_names = feature_names","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calcMetrics(y_test, y_pred, results, i, numl, st, isBin, y_test_p=0, preds=0):\n  if isBin: #binary\n    print(Fore.RED + \"Binary metrics\"+Fore.BLACK)\n    results.at[i*numl+st,'Precision'] = sklearn.metrics.precision_score(y_test, y_pred)\n    results.at[i*numl+st,'Recall'] = sklearn.metrics.recall_score(y_test, y_pred)\n    results.at[i*numl+st, 'F-measure'] = sklearn.metrics.f1_score(y_test, y_pred)\n    results.at[i*numl+st,'AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred)\n  else: #multiclass\n    print(Fore.RED + \"MultiClass metrics\"+Fore.BLACK)\n    results.at[i*numl+st,'Precision'] = sklearn.metrics.precision_score(y_test, y_pred, average='macro', zero_division=0)\n    results.at[i*numl+st,'Recall'] = sklearn.metrics.recall_score(y_test, y_pred, average='macro', zero_division=0)\n    results.at[i*numl+st,'F-measure'] = sklearn.metrics.f1_score(y_test, y_pred, average='macro')\n    results.at[i*numl+st,'AUC'] = sklearn.metrics.roc_auc_score(y_test_p, preds, multi_class='ovr')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fillCommon(dataset, name, time, autoML, results, i, num, st):\n  results.at[i*num+st,'ID'] = dataset\n  results.at[i*num+st,'DataSet'] = name\n  results.at[i*num+st,'Time lim.'] = time\n  results.at[i*num+st,'AutoML lib.'] = autoML","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#method for starting AutoSklearn\n#dataset - data to process, time - running time limit, ratio - split ration of train/test, i - number of run, numl - number of used libraries, st - start (in table of results)\ndef runAutoSki(dataset, timelim, ratio, i, numl, st):   #i*numl+st\n  if __name__ == '__main__':\n    X = dataset.data\n    y = dataset.target\n    print(\"type\", y.dtype)\n    print(\"A-S X \", X)\n    print(\"A-S y \", y)\n    if len(y.unique()) ==2: #afterwards y will be encoded!!!\n      print(Fore.RED + \"running AutoSklearn Binary\"+Fore.BLACK)\n      bin = True\n    else:\n      print(Fore.RED + \"running AutoSklearn MultiClass\"+Fore.BLACK)\n      bin = False\n\n    if y.dtype=='category' or y.dtype=='object':\n      print(\"categorial class encoding\")\n      #encoding for text values!!!\n      le = preprocessing.LabelEncoder()\n      y = le.fit_transform(y)\n\n    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=ratio, random_state=1)\n    cls = autosklearn.classification.AutoSklearnClassifier(ensemble_class=None, time_left_for_this_task=timelim)\n\n    print(Fore.RED + \"Search for AutoSklearn model\"+Fore.BLACK)\n    start_time1 = time.time()\n    cls.fit(X_train, y_train)\n    elapsed1 = time.time() - start_time1\n    print(Fore.RED + \"AutoSklearn model found\"+Fore.BLACK)\n\n    predictions = cls.predict(X_test)\n\n    results.at[i*numl+st,'Algorithm'] = cls.leaderboard().iat[0,2]\n    results.at[i*numl+st,'Accuracy'] = cls.score(X_test, y_test)\n    results.at[i*numl+st,'Time'] = elapsed1\n\n    if bin:\n      calcMetrics(y_test, predictions, results, i, numl, st, True)\n    else:\n      pred_proba = cls.predict_proba(X_test)  # for muliclass tasks\n      calcMetrics(y_test, predictions, results, i, numl, st, False, y_test, pred_proba)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#method for starting H2O\ndef runH2O(dataset, timelim, ratio, i, numl, st):\n  x=dataset.feature_names\n  y=dataset.target.name\n\n  if len(dataset.target.unique())==2:\n    print(Fore.RED + \"running H2O Binary\"+Fore.BLACK)\n  else:\n    print(Fore.RED + \"running H2O MultiClass\"+Fore.BLACK)\n\n  #h2o.init()\n  frame = h2o.H2OFrame(dataset.frame)\n  frame[y] = frame[y].asfactor()\n  train, test = frame.split_frame(ratios=[ratio])\n\n  automl = h2o.automl.H2OAutoML(max_runtime_secs=timelim)\n  print(Fore.RED + \"Search for H2O model\"+Fore.BLACK)\n  start_time2 = time.time()\n  automl.train(x=x, y=y, training_frame=train)\n  elapsed2 = time.time() - start_time2\n  print(Fore.RED + \"H2O model found\"+Fore.BLACK)\n\n  perf = automl.leader.model_performance(test)\n  print(\"perf type:\", type(perf))\n\n  results.at[i*numl+st,'Algorithm'] = automl.leader.algo\n  results.at[i*numl+st,'Time'] = elapsed2\n\n  if len(dataset.target.unique())==2: #binary\n    results.at[i*numl+st,'Accuracy'] = perf.accuracy()[0][1]\n    results.at[i*numl+st,'Precision'] = perf.precision()[0][1]\n    results.at[i*numl+st,'Recall'] = perf.recall()[0][1]\n    results.at[i*numl+st,'F-measure'] = perf.F1()[0][1]\n    results.at[i*numl+st,'AUC'] = perf.auc()  #[0][1]\n  else: #multiclass\n    y_test = test[y].as_data_frame() #for accuracy calculation\n    y_pred_0 = automl.leader.predict(test)  #calculate predictions\n    y_pred = y_pred_0['predict'].as_data_frame() #convert column with predictions to dataframe\n    results.at[i*numl+st,'Accuracy'] = sklearn.metrics.accuracy_score(y_test, y_pred)\n\n    preds_0 = y_pred_0.as_data_frame() #convert H2OFrame to DataFrame\n    preds = preds_0.drop('predict', axis=1)  #remove name of classes (first column)\n    y_test_p = y_test[y]  #convert DataFrame to Series\n    calcMetrics(y_test, y_pred, results, i, numl, st, False, y_test_p, preds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#method for starting AutoML libs in parallel\ndef runAutoML(dataset, source, time, ratio, i, libs=[]):\n  print(Fore.RED + \"starting AutoML num \", i)\n  print(Fore.BLACK)\n\n  if source=='oml': #type(dataset) == int:\n    print(\"OpenML. fetch by id\")\n    opml = fetch_openml(data_id=dataset, as_frame=True)\n    name = opml.details['name']\n    #-------\n    #opml_ds = openml.datasets.get_dataset(dataset, download_data=True)\n    #info, _, _, features = opml_ds.get_data(dataset_format=\"dataframe\")\n    #features.remove(opml_ds.default_target_attribute)\n    #X = info[features]\n    #y = info[opml_ds.default_target_attribute]\n    #name = opml_ds.name #opml.details['name']\n    #opml = Info(X, y, info, features)\n    #--------\n  elif source=='uci':\n    print(\"UCI Repository\")\n    ucir = fetch_ucirepo(id=dataset)\n    opml = Info(ucir.data.features, ucir.data.targets[ucir.metadata['target_col'][0]], ucir.data.original, list(ucir.data.features.columns))\n    name = ucir.metadata['name']\n  elif source=='kag':\n    print(\"kaggle\")\n  else:\n    if type(dataset) != int:\n      print(\"text. fetch from file\")\n      #load dataset from file\n      info = pd.read_csv(dataset, delimiter=';')\n      name = dataset\n      #info = pd.read_csv(dataset)\n      y = info[\"PerformanceRating\"] # info[info.columns[info.columns.size-1]]\n      X = info.drop(\"PerformanceRating\", axis=1) #info[info.columns.delete(info.columns.size-1)]\n      #print(\"type: \", type(info))\n      print(\"X: \", X)\n      print(\"y: \", y)\n      #print(\"info columns:\", info.columns.drop(\"PerformanceRating\"))\n      feature_names = list(info.columns.drop(\"PerformanceRating\"))\n      target_name = \"PerformanceRating\"\n      opml = Info(X, y, info, feature_names)\n\n  print(\"X:\", len(opml.feature_names))\n  print(\"y:\", opml.target)\n\n  numl = len(libs)\n  t1b = False #flag if AS started\n  t2b = False #flag if H2O started\n  #t3b = False #flag if FLAML started #FLAML not suitable!!\n\n  if 'AS' in libs:\n    autoML = 'AutoSklearn'\n    st = libs.index('AS')\n    fillCommon(dataset, name, time, autoML, results, i, numl, st)\n\n    print(Fore.RED + 'Start Auto-sklearn', st)\n    print(Fore.BLACK)\n    t1b = True\n    # creating thread\n    t1 = threading.Thread(target=runAutoSki, args=(opml, time, 1-ratio, i, numl, st))\n    t1.start()\n  if 'H2O' in libs:\n    autoML = 'H2O'\n    st = libs.index('H2O')\n    fillCommon(dataset, name, time, autoML, results, i, numl, st)\n    print(Fore.RED + 'Start H2O', st)\n    print(Fore.BLACK)\n    t2b = True\n    # creating thread\n    t2 = threading.Thread(target=runH2O, args=(opml, time, ratio, i, numl, st))\n    t2.start()\n\n  if len(libs)==0:\n    numl = 2 #we have 2 libs here in total\n    print(Fore.RED + 'Start All'+Fore.BLACK)\n    t1b = True #flag if AS started\n    autoML = 'AutoSklearn'\n    st = 0\n    fillCommon(dataset, name, time, autoML, results, i, numl, st)\n    # creating thread\n    t1 = threading.Thread(target=runAutoSki, args=(opml, time, 1-ratio, i, numl, st))\n    t1.start()\n\n    t2b = True #flag if H2O started\n    autoML = 'H2O'\n    st = 1\n    fillCommon(dataset, name, time, autoML, results, i, numl, st)\n    # creating thread\n    t2 = threading.Thread(target=runH2O, args=(opml, time, ratio, i, numl, st))\n    t2.start()\n\n  # wait until threads are completely executed\n  if t1b:\n    print(Fore.RED + \"t1 exists and needs to join!\"+Fore.BLACK)\n    t1.join()\n  if t2b:\n    print(Fore.RED + \"t2 exists and needs to join!\"+Fore.BLACK)\n    t2.join()\n\n  print(\"results: \", results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"h2o.init()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"runAutoML(1590, \"oml\", 300, 0.75, i, ['AS', 'H2O'])\ni = i + 1 #for filling next rows in table with results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(results)\nresults.to_excel('/kaggle/working/AutoSklearn-H2O.FW.results.xlsx')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}